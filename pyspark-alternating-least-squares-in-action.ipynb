{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Continuing exploring restaurant customers\nIn the notebook <a href='https://www.kaggle.com/erelin6613/customer-is-always-right'>Customer is always right</a> we did some basic analysis based on a sample of data. It is usually the case we have wast amount of data, often even sparse data when dealing with recommendation problem. Luckily for us, we have a few tools whcih come to resque.","metadata":{}},{"cell_type":"code","source":"!pip3 install pyspark --quiet","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pyspark as spark\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc = spark.SparkContext()\nsql = spark.sql.SQLContext(sc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = '../input/restaurant-recommendation-challenge'\nfiles = dict()\nfor f in os.listdir(root_dir):\n    if f.endswith('.csv'):\n        files[f] = sql.read.format('csv').options(header='true').load(os.path.join(root_dir, f))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files['train_full.csv'].groupBy('discount_percentage').count().orderBy('count').show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files['train_full.csv'].groupby('target').count().orderBy('count').show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prep_s = files['train_full.csv'].select('prepration_time').toPandas()\nprep_s['prepration_time'].astype('float').hist(color='gold')\ndel prep_s","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_method = files['orders.csv'].select('payment_mode').toPandas()\np_method['payment_mode'].astype('float').hist()\ndel p_method","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total = files['orders.csv'].select('grand_total').toPandas()\nsns.distplot(total['grand_total'].astype('float'), color='purple')\ndel total","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope this will be enough to ensure our sampled distributions are roughly the same on larger scale too.","metadata":{}},{"cell_type":"markdown","source":"### Preprocessing\n\nWe will need to dive into data once again to extract features we need but now we will handle it with pyspark. This is what we will start with next time.","metadata":{}},{"cell_type":"code","source":"train_full_drop = ['country_id', 'commission', 'display_orders']\norders_drop = ['akeed_order_id']\ntrain_customers_drop = ['language']","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}